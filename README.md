# Named Entity Extraction Pipeline with NLP + Azure

This project implements an end-to-end Named Entity Recognition (NER) pipeline using a fine-tuned spaCy model & BERT Model, Azure Blob Storage for document ingestion, Azure Cosmos DB for storage, and FastAPI for serving results via an API, Azure App Services & Github Actions for deployment & scaling.

---

## ğŸš€ Features
- Load documents from Azure Blob Storage
- Extract named entities using a custom spaCy NER model
- Store raw + enriched documents in Azure Cosmos DB
- Expose extracted entities via a FastAPI API endpoint
- Search entities by label, date, and keyword

---

## ğŸ“ Project Structure
```
EntityExtractionDemo/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ service/
â”‚   â”‚   â”œâ”€â”€ main.py           # FastAPI app
â”‚   â”‚   â”œâ”€â”€ db.py             # Cosmos DB interaction
â”‚   â”‚   â”œâ”€â”€ data_collection_utility.py    # Blob ingestion logic
â”‚   â”‚   â”œâ”€â”€ ner.py            # spaCy NER logic
â”‚   â”‚   â”œâ”€â”€ search.py         # Search API
â”‚   â”‚   |â”€â”€ run_pipeline.py   # Batch processor
â”‚   â””â”€â”€ __init__.py
â”œ   |
|   â””â”€â”€ model/output         # Trained spaCy model
â”œâ”€â”€ .env                      # Environment variables
â”œâ”€â”€ Dockerfile                # Docker config
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ .github/workflows/deploy.yml # GitHub Actions CI : to azure App Service
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/sofiahalima/named_entity_extraction_01.git
cd named_entity_extraction_01
```

### 2. Create and Activate a Virtual Environment
```bash
python -m venv .venv
source .venv/bin/activate  
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Create a `.env` File
```
AZURE_BLOB_CONNECTION_STRING=your-connection-string
AZURE_BLOB_CONTAINER_NAME=your-container-name
COSMOS_DB_URL=your-cosmos-url
COSMOS_DB_KEY=your-cosmos-key
COSMOS_DB_NAME=ner_pipeline
COSMOS_CONTAINER_NAME=documents
PROJECT_ROOT = your root folder
```

---

## ğŸ“¦ Run the NER Pipeline
```bash
python pipeline/run_pipeline.py
```

---

## ğŸŒ Start the FastAPI Server
```bash
python -m uvicorn app.service.main:app --reload
```

Then open: [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

### Example API Calls
- `GET /entities?id=doc-123` â€“ Get NER results by document ID
- `GET /search?label=person&start_date=2025-01-01&keyword=climate` â€“ Search by filters :TODO( Search by keyword & date)

---

## ğŸ³ Docker Usage :TODO (planned to deploy to Azure App Services)
```bash
docker build -t ner-api .
docker run -p 8000:8000 --env-file .env ner-api
```

---

## âœ… GitHub Actions CI/CD
- Located at `.github/workflows/deploy.yml`  
- Linting and dependency check on every `.github/workflows/lint.yml`

---

## ğŸ“Œ Notes
- Make sure your spaCy model is saved to `app/model/output/`
- All folders contain `__init__.py` to ensure package resolution
- Supports Cosmos DB SQL API only

---
